{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "## IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import wandb\n",
    "import requests,zipfile,io\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOWNLOADING AND UNZIPPING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # Importing the requests library to make HTTP requests\n",
    "import zipfile   # Importing the zipfile library to handle zip files\n",
    "import io        # Importing the io library for input/output operations\n",
    "\n",
    "def download_data(url=\"https://drive.usercontent.google.com/u/0/uc?id=1tGIO4-IPNtxJ6RQMmykvAfY_B0AaLY5A&export=download\"):\n",
    "    \n",
    "    # Make an HTTP GET request to the specified URL and store the response\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Create a ZipFile object from the response content\n",
    "    z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "\n",
    "    # Extract all the contents of the zip file\n",
    "    z.extractall()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METHODS FOR GETTING CHARACTERS FOR CORPUSS AND ADDING THEIR INDICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_chars(word,st):\n",
    "    st.add(ch for ch in word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entire_collection(data):\n",
    "    eng_corpus = set()  # Set to store English characters\n",
    "    hin_corpus = set()  # Set to store Hindi characters\n",
    "    \n",
    "    for i in range(0, len(data)):\n",
    "        \n",
    "        # Add each character of the English word to the English corpus set\n",
    "        add_chars(data[0][i],eng_corpus)\n",
    "        \n",
    "        # Add each character of the Hindi word to the Hindi corpus set\n",
    "        add_chars(data[1][i],hin_corpus)\n",
    "        \n",
    "    # Add end delimiter characters to both corpora\n",
    "    eng_corpus.add('#')\n",
    "    hin_corpus.add('#')\n",
    "    hin_corpus.add('$')\n",
    "    eng_corpus.add('$')\n",
    "    \n",
    "    # Add start delimiter character to the Hindi corpus\n",
    "    hin_corpus.add('^')\n",
    "    \n",
    "    return eng_corpus, len(eng_corpus), hin_corpus, len(hin_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMapping(corpus):\n",
    "    chrToIndex = {}\n",
    "    idxToChr = {}\n",
    "    for i,char in enumerate(corpus):\n",
    "        chrToIndex[char]=i\n",
    "        idxToChr[i]=char\n",
    "    return chrToIndex, idxToChr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hindi training csv\n",
    "def word2index(data):\n",
    "    eng_corpus, eng_vocab_size, hin_corpus, hin_vocab_size = get_entire_collection(data)  # Get Hindi and English corpora from data and their respective counts\n",
    "    engchar_idx, idx_engchar = createMapping(eng_corpus) # Dictionary to map English characters to indices and indices to English characters\n",
    "    hinchar_idx, idx_hinchar = createMapping(hin_corpus) # Dictionary to map Hindi characters to indices and indices to Hindi character\n",
    "    return eng_vocab_size, hin_vocab_size, engchar_idx, hinchar_idx, idx_engchar, idx_hinchar\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMax(data):\n",
    "    mx = 0\n",
    "    for word in data:\n",
    "        # Update mxif the length of word is greater\n",
    "        mx=max(mx,len(word))\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxlen(data):        \n",
    "    maxlen_eng = getMax(data[0]) # Variable to store the maximum length of English words\n",
    "    maxlen_hin = getMax(data[1]) # Variable to store the maximum length of Hindi words\n",
    "    return maxlen_eng, maxlen_hin\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def padding(data,lst,mapping):\n",
    "#     for word in data:\n",
    "def util_preprocess(data, maxLen, unKnown, word_to_idx, hindi):\n",
    "    sentence = []\n",
    "    for word in data:\n",
    "        if(hindi):\n",
    "            word = '^' + word # Add start delimiter (^) to Hindi word\n",
    "        # Pad the words to their respective maximum lengths\n",
    "        word = word.ljust(maxLen+1, '#')\n",
    "        idx = []\n",
    "        for ch in word:\n",
    "            if(ch in word_to_idx):\n",
    "                idx.append(word_to_idx[ch])\n",
    "            else:\n",
    "                idx.append(unKnown)\n",
    "        sentence.append(idx)\n",
    "    return sentence\n",
    "    \n",
    "        \n",
    "def pre_process(data, eng_to_idx, hin_to_idx):\n",
    "    \n",
    "    maxlen_eng, maxlen_hin = getMax(data[0]), getMax(data[1])  # Get the maximum lengths of English and Hindi words\n",
    "       \n",
    "    unKnown = eng_to_idx['$']  # Index for unknown character in English corpus\n",
    "\n",
    "    eng = util_preprocess(data[0],maxlen_eng,unKnown,eng_to_idx,False) # List to store pre-processed English sentences\n",
    "    hin = util_preprocess(data[1],maxlen_hin,unKnown,hin_to_idx,True) # List to store pre-processed Hindi sentences\n",
    "    \n",
    "    return eng, hin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOADING OUR CUSTOM DATASET TO DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, train_x, train_y, transform=None):\n",
    "        self.transform = transform  # Optional data transformation\n",
    "        self.train_y = train_y  # Target data (train_y)\n",
    "        self.train_x = train_x  # Input data (train_x)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)  # Apply the transformation (if any) to the sample\n",
    "            \n",
    "        # Return the input and target data tensors for the given index\n",
    "        return torch.tensor(self.train_x[idx]).to(device), torch.tensor(self.train_y[idx]).to(device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_x)  # Return the length of the dataset\n",
    "\n",
    "def get_data(downloaded=False):\n",
    "    # if(not downloaded):\n",
    "    # download_data()  # Download the data (assuming it has been implemented elsewhere)\n",
    "    \n",
    "    # Read the train, test, and validation datasets from CSV files\n",
    "    test_df = pd.read_csv(\"aksharantar_sampled/hin/hin_test.csv\", header=None)\n",
    "    train_df = pd.read_csv(\"aksharantar_sampled/hin/hin_train.csv\", header=None)\n",
    "    val_df = pd.read_csv(\"aksharantar_sampled/hin/hin_valid.csv\", header=None)\n",
    "    \n",
    "    # Convert words to indices and retrieve vocabulary information\n",
    "    input_len, target_len, eng_to_idx, hin_to_idx, idx_to_eng, idx_to_hin = word2index(train_df)\n",
    "    \n",
    "    # Return the datasets and vocabulary information\n",
    "    return train_df, test_df, val_df, eng_to_idx, hin_to_idx, idx_to_eng, idx_to_hin, input_len, target_len\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderGRU(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,embedding_size,num_of_layers,batch_size,bi_directional,dropout_p=0.1):\n",
    "        super(EncoderGRU,self).__init__()\n",
    "        self.batch_size=batch_size\n",
    "        self.num_of_layers=num_of_layers\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, num_of_layers, bidirectional = bi_directional==\"Yes\")\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.bi_directional=bi_directional\n",
    "        self.embedding=nn.Embedding(input_size,embedding_size)\n",
    "        self.hidden_size=hidden_size\n",
    "        self.embedding_size=embedding_size\n",
    "        self.input_size=input_size\n",
    "        \n",
    "\n",
    "    def forward(self,input,hidden):\n",
    "        embedded=self.embedding(input).view(-1,self.batch_size, self.embedding_size)\n",
    "        embedded = self.dropout(embedded)\n",
    "        output,hidden=self.gru(embedded,hidden)\n",
    "    \n",
    "        if self.bi_directional==\"Yes\":\n",
    "            hidden=hidden.resize(2,self.num_of_layers,self.batch_size,self.hidden_size)\n",
    "            hidden=torch.add(hidden[0],hidden[1])/2\n",
    "            \n",
    "        return output,hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        if(self.bi_directional!=\"Yes\"):\n",
    "            return torch.zeros(self.num_of_layers,self.batch_size,self.hidden_size,device=device)\n",
    "        else:\n",
    "            return torch.zeros(2*self.num_of_layers,self.batch_size,self.hidden_size,device=device)\n",
    "\n",
    "class DecoderGRU(nn.Module):\n",
    "    def __init__(self, output_size,hidden_size, embedding_size, decoder_layers,batch_size,dropout_p=0.1):\n",
    "        super(DecoderGRU, self).__init__()\n",
    "        self.embedding_size=embedding_size\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        self.batch_size=batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(embedding_size,hidden_size, decoder_layers,dropout = dropout_p)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(-1, self.batch_size, self.embedding_size)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        output = self.softmax(self.out(output))\n",
    "        return output, hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,embedding_size,num_of_layers,batch_size,bi_directional,dropout_p=0.1):\n",
    "        super(EncoderRNN,self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_of_layers=num_of_layers\n",
    "        self.batch_size=batch_size\n",
    "        self.embedding=nn.Embedding(input_size,embedding_size)\n",
    "        self.bi_directional=bi_directional\n",
    "        self.input_size=input_size\n",
    "        self.rnn = nn.RNN(embedding_size, hidden_size, num_of_layers, bidirectional = bi_directional!=\"No\")\n",
    "        self.embedding_size=embedding_size\n",
    "\n",
    "    def forward(self,input,hidden):\n",
    "        embedded=self.embedding(input).view(-1,self.batch_size, self.embedding_size)\n",
    "        embedded = self.dropout(embedded)\n",
    "        output,hidden=self.rnn(embedded,hidden)\n",
    "    \n",
    "        if(self.bi_directional==\"Yes\"):\n",
    "            hidden=hidden.resize(2,self.num_of_layers,self.batch_size,self.hidden_size)\n",
    "            hidden=torch.add(hidden[0],hidden[1])/2\n",
    "            \n",
    "        return output,hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        if(self.bi_directional!=\"Yes\"):\n",
    "            return torch.zeros(self.num_of_layers,self.batch_size,self.hidden_size,device=device)\n",
    "        else:\n",
    "            return torch.zeros(2*self.num_of_layers,self.batch_size,self.hidden_size,device=device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, output_size,hidden_size, embedding_size, decoder_layers,batch_size,dropout_p=0.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(embedding_size,hidden_size, decoder_layers,dropout = dropout_p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "        self.batch_size=batch_size\n",
    "        self.embedding_size=embedding_size\n",
    "        \n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(-1, self.batch_size, self.embedding_size)\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        output = self.softmax(self.out(output))\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,embedding_size,num_of_layers,batch_size,bi_directional,dropout_p=0.1):\n",
    "        super(EncoderLSTM,self).__init__()\n",
    "        self.num_of_layers=num_of_layers\n",
    "        self.embedding_size=embedding_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.input_size=input_size\n",
    "        self.embedding=nn.Embedding(input_size,embedding_size) \n",
    "        self.bi_directional=bi_directional\n",
    "        self.lstm = nn.LSTM(embedding_size,hidden_size,num_of_layers,bidirectional = bi_directional==\"Yes\")\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.batch_size=batch_size\n",
    "\n",
    "    def forward(self,input,hidden,state):\n",
    "        embedded=self.embedding(input).view(-1,self.batch_size, self.embedding_size)\n",
    "        embedded = self.dropout(embedded)\n",
    "        output,(hidden,state)=self.lstm(embedded,(hidden,state))\n",
    "    \n",
    "        if(self.bi_directional!=\"No\"):\n",
    "            hidden=hidden.resize(2,self.num_of_layers,self.batch_size,self.hidden_size)\n",
    "            state=state.resize(2,self.num_of_layers,self.batch_size,self.hidden_size)\n",
    "            hidden=torch.add(hidden[0],hidden[1])/2\n",
    "            state=torch.add(state[0],hidden[1])/2\n",
    "            \n",
    "        return output,hidden,state\n",
    "\n",
    "\n",
    "\n",
    "    def initHidden(self):\n",
    "        if(self.bi_directional!=\"Yes\"):\n",
    "            return torch.zeros(self.num_of_layers,self.batch_size,self.hidden_size,device=device)\n",
    "        else:\n",
    "            return torch.zeros(2*self.num_of_layers,self.batch_size,self.hidden_size,device=device)      \n",
    "    \n",
    "    def initState(self):\n",
    "        if(self.bi_directional!=\"Yes\"):\n",
    "            return torch.zeros(self.num_of_layers,self.batch_size,self.hidden_size,device=device)\n",
    "        else:\n",
    "            return torch.zeros(2*self.num_of_layers,self.batch_size,self.hidden_size,device=device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, output_size,hidden_size, embedding_size, decoder_layers,batch_size,dropout_p=0.1):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size=embedding_size        \n",
    "        self.lstm = nn.LSTM(embedding_size,hidden_size,decoder_layers,dropout = dropout_p)\n",
    "        self.batch_size=batch_size\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input,hidden,state):\n",
    "        embedded = self.embedding(input).view(-1, self.batch_size, self.embedding_size)\n",
    "        output,(hidden,state)=self.lstm(embedded,(hidden,state))\n",
    "        output = self.softmax(self.out(output))\n",
    "        return output,hidden,state\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATTENTION MECHANISM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoder(nn.Module):\n",
    "    def __init__(self,output_size,hidden_size,embedding_size,decoder_layers,batch_size,cell_type,dropout_p=0.1):\n",
    "        super(AttnDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size=embedding_size\n",
    "        self.decoder_layers=decoder_layers\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.batch_size=batch_size\n",
    "        self.cell_type=cell_type\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        \n",
    "\n",
    "        self.U=nn.Linear(hidden_size,hidden_size,bias=False).to(device)\n",
    "        self.V=nn.Linear(hidden_size,1,bias=False).to(device)\n",
    "        self.W=nn.Linear(hidden_size,hidden_size,bias=False).to(device)\n",
    "        \n",
    "        self.linear=nn.Linear(self.hidden_size,output_size,bias=True)\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        self.softmax1=nn.LogSoftmax(dim=2)\n",
    "\n",
    "        if(cell_type==\"RNN\"):\n",
    "            self.rnn = nn.RNN(self.embedding_size+self.hidden_size, self.hidden_size,self.decoder_layers,dropout = dropout_p)\n",
    "            \n",
    "        if(cell_type==\"LSTM\"):\n",
    "            self.lstm = nn.LSTM(self.embedding_size+self.hidden_size, self.hidden_size,self.decoder_layers,dropout = dropout_p)\n",
    "            \n",
    "        if(cell_type==\"GRU\"):\n",
    "            self.gru = nn.GRU(self.embedding_size+self.hidden_size, self.hidden_size,self.decoder_layers,dropout = dropout_p)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input, hidden,encoder_outputs,word_length,state=None):\n",
    "        embedded = self.embedding(input).view(-1,self.batch_size, self.embedding_size)\n",
    "        T=word_length\n",
    "        temp1=self.W(hidden[-1])\n",
    "        temp2=self.U(encoder_outputs)\n",
    "        c=torch.zeros(self.batch_size,1,self.hidden_size).to(device)\n",
    "        temp1=temp1.unsqueeze(0)\n",
    "\n",
    "        e_j=self.V(F.tanh(temp1+temp2))\n",
    "        alpha_j=self.softmax(e_j)\n",
    "        \n",
    "        c = torch.bmm(alpha_j.permute(1,2,0),encoder_outputs.permute(1,0,2))\n",
    "        \n",
    "        final_input=torch.cat((embedded[0],c.squeeze(1)),1).unsqueeze(0)\n",
    "    \n",
    "        final_input = F.relu(final_input)\n",
    "        \n",
    "        \n",
    "        if(self.cell_type==\"RNN\"):\n",
    "            output,hidden=self.rnn(final_input,hidden)\n",
    "        elif(self.cell_type==\"LSTM\"):\n",
    "            output, (hidden,state) =self.lstm(final_input,(hidden,state))\n",
    "        else:\n",
    "            output,hidden=self.gru(final_input,hidden)\n",
    "        \n",
    "        output1=self.softmax1(self.linear(output))\n",
    "        \n",
    "        if(self.cell_type==\"LSTM\"):\n",
    "            return output1, hidden, state, alpha_j\n",
    "        else:\n",
    "            return output1, hidden, alpha_j\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_util1(decoder_layers,encoder_layers,decoder_hidden,decoder_state):\n",
    "    i = decoder_layers\n",
    "    while(i>encoder_layers):\n",
    "        if(i==encoder_layers):\n",
    "            break\n",
    "        # Concatenate the two tensors along the first dimension\n",
    "        decoder_hidden = torch.cat([decoder_hidden, encoder_hidden[-1].unsqueeze(0)], dim=0)\n",
    "        decoder_state = torch.cat([decoder_state, encoder_state[-1],unsqueeze(0)], dim=0)\n",
    "        i-=1\n",
    "    return decoder_hidden, decoder_state\n",
    "\n",
    "\n",
    "\n",
    "def train(train_data, encoder, decoder, loss_fun, hidden_size, bi_directional, cell_type, attention, encoder_optimizer, decoder_optimizer, encoder_layers, decoder_layers, batch_size):\n",
    "    total_loss = 0\n",
    "    teacher_forcing_ratio = 0.5\n",
    "    \n",
    "    # Iterate over the training data\n",
    "    for i, (train_x, train_y) in enumerate(train_data):\n",
    "        loss = 0\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        # Transposing the dataset\n",
    "        train_x = train_x.T\n",
    "        train_y = train_y.T\n",
    "        timesteps = len(train_x)\n",
    "        \n",
    "        \n",
    "        # Check the cell type (LSTM)\n",
    "        if cell_type == 'LSTM':\n",
    "            encoder_hidden = encoder.initHidden()\n",
    "            encoder_state = encoder.initState()\n",
    "            \n",
    "            encoder_output, encoder_hidden, encoder_state = encoder(train_x, encoder_hidden, encoder_state)\n",
    "        \n",
    "            if decoder_layers > encoder_layers:\n",
    "                i = decoder_layers\n",
    "                decoder_hidden = encoder_hidden\n",
    "                decoder_state = encoder_state\n",
    "                \n",
    "                while True:\n",
    "                    if i == encoder_layers:\n",
    "                        break\n",
    "                    # Concatenate the two tensors along the first dimension\n",
    "                    decoder_hidden = torch.cat([decoder_hidden, encoder_hidden[-1].unsqueeze(0)], dim=0)\n",
    "                    decoder_state = torch.cat([decoder_state, encoder_state[-1].unsqueeze(0)], dim=0)\n",
    "                    i -= 1\n",
    "            elif decoder_layers < encoder_layers:\n",
    "                decoder_hidden = encoder_hidden[-decoder_layers:]\n",
    "                decoder_state = encoder_state[-decoder_layers:]\n",
    "            else:\n",
    "                decoder_hidden = encoder_hidden\n",
    "                decoder_state = encoder_state\n",
    "            \n",
    "            if bi_directional != \"No\":\n",
    "                split_tensor = torch.split(encoder_output, hidden_size, dim=-1)\n",
    "                encoder_output = torch.add(split_tensor[0], split_tensor[1]) / 2\n",
    "            \n",
    "            decoder_input = train_y[0]\n",
    "            use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "            if(not use_teacher_forcing):\n",
    "                for i in range(0, len(train_y)):\n",
    "                    if attention == \"Yes\":\n",
    "                        decoder_output, decoder_hidden, decoder_state, attn_weights = decoder(decoder_input, decoder_hidden, encoder_output, len(train_x), decoder_state)\n",
    "                    else:\n",
    "                        decoder_output, decoder_hidden, decoder_state = decoder(decoder_input, decoder_hidden, decoder_state)\n",
    "                    max_prob, index = decoder_output.topk(1)\n",
    "                    loss += loss_fun(torch.squeeze(decoder_output), train_y[i])\n",
    "                    decoder_input = index\n",
    "            else:\n",
    "                for i in range(0, len(train_y)):\n",
    "                    if attention == \"Yes\":\n",
    "                        decoder_output, decoder_hidden, decoder_state, attn_weights = decoder(decoder_input, decoder_hidden, encoder_output, len(train_x), decoder_state)\n",
    "                        # loss += loss_fun(torch.squeeze(decoder_output), train_y[i])\n",
    "                        # decoder_input = train_y[i]\n",
    "                    else:\n",
    "                        decoder_output, decoder_hidden, decoder_state = decoder(decoder_input, decoder_hidden, decoder_state)\n",
    "                    loss += loss_fun(torch.squeeze(decoder_output), train_y[i])\n",
    "                    decoder_input = train_y[i]  # Teacher forcing\n",
    "            \n",
    "            \n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            total_loss += loss\n",
    "            \n",
    "        # Check the cell type (RNN, GRU, LSTM)\n",
    "        if cell_type == 'GRU' or cell_type == 'RNN':\n",
    "            # Initialize the hidden state of the encoder\n",
    "            encoder_hidden = encoder.initHidden()\n",
    "            \n",
    "            # Pass the input through the encoder\n",
    "            encoder_output, encoder_hidden = encoder(train_x, encoder_hidden)\n",
    "            \n",
    "            # Adjust decoder hidden state based on the number of layers\n",
    "            if decoder_layers > encoder_layers:\n",
    "                i = decoder_layers\n",
    "                decoder_hidden = encoder_hidden\n",
    "                \n",
    "                while True:\n",
    "                    if i == encoder_layers:\n",
    "                        break\n",
    "                    # Concatenate the two tensors along the first dimension\n",
    "                    decoder_hidden = torch.cat([decoder_hidden, encoder_hidden[-1].unsqueeze(0)], dim=0)\n",
    "                    i -= 1\n",
    "            elif decoder_layers < encoder_layers:\n",
    "                decoder_hidden = encoder_hidden[-decoder_layers:]\n",
    "            else:\n",
    "                decoder_hidden = encoder_hidden\n",
    "        \n",
    "            decoder_input = train_y[0]\n",
    "            \n",
    "            # Apply bidirectional averaging if specified\n",
    "            if bi_directional == \"Yes\":\n",
    "                split_tensor = torch.split(encoder_output, hidden_size, dim=-1)\n",
    "                encoder_output = torch.add(split_tensor[0], split_tensor[1]) / 2\n",
    "            \n",
    "            # Determine whether to use teacher forcing\n",
    "            use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            if(not use_teacher_forcing):\n",
    "                # Without teacher forcing: use the predicted output as the next input\n",
    "                for i in range(0, len(train_y)):\n",
    "                    if attention == \"Yes\":\n",
    "                        decoder_output, decoder_hidden, attn_weights = decoder(decoder_input, decoder_hidden, encoder_output, len(train_x))\n",
    "                        # max_prob, index = decoder_output.topk(1)\n",
    "                        # loss += loss_fun(torch.squeeze(decoder_output), train_y[i])\n",
    "                        # decoder_input = index\n",
    "                    else:\n",
    "                        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                    max_prob, index = decoder_output.topk(1)\n",
    "                    loss += loss_fun(torch.squeeze(decoder_output), train_y[i])\n",
    "                    decoder_input = index\n",
    "                        \n",
    "            else:\n",
    "                # Teacher forcing: feed the target as the next input\n",
    "                for i in range(0, len(train_y)):\n",
    "                    if attention == \"Yes\":\n",
    "                        # Pass input, hidden state, and encoder output through the decoder with attention\n",
    "                        decoder_output, decoder_hidden, attn_weights = decoder(decoder_input, decoder_hidden, encoder_output, len(train_x))\n",
    "                        # loss += loss_fun(torch.squeeze(decoder_output), train_y[i])\n",
    "                        # decoder_input = train_y[i]\n",
    "                    else:\n",
    "                        # Pass input and hidden state through the decoder\n",
    "                        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                    loss += loss_fun(torch.squeeze(decoder_output), train_y[i])\n",
    "                    decoder_input = train_y[i]  # Teacher forcing\n",
    "            \n",
    "            \n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            total_loss += loss\n",
    "        \n",
    "        \n",
    "    return total_loss.item() / len(train_y), encoder, decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iter(input_data, val_data, val_y, input_len, hidden_size, cell_type, bi_directional, dropout, attention, target_len, epochs, batch_size, embedding_size, encoder_layers, decoder_layers):\n",
    "    lr = 0.001\n",
    "    beam_size=0\n",
    "    # Initialize the encoder and decoder based on the cell type and attention\n",
    "    if(cell_type == 'RNN'):\n",
    "        encoder = EncoderRNN(input_len, hidden_size, embedding_size, encoder_layers, batch_size, bi_directional, dropout).to(device)\n",
    "        if(attention!='Yes'):\n",
    "            decoder = DecoderRNN(target_len, hidden_size, embedding_size, decoder_layers, batch_size, dropout).to(device)\n",
    "    elif(cell_type == 'GRU'):\n",
    "        encoder = EncoderGRU(input_len, hidden_size, embedding_size, encoder_layers, batch_size, bi_directional, dropout).to(device)\n",
    "        if(attention!='Yes'):\n",
    "            decoder = DecoderGRU(target_len, hidden_size, embedding_size, decoder_layers, batch_size, dropout).to(device)\n",
    "    else:\n",
    "        encoder = EncoderLSTM(input_len, hidden_size, embedding_size, encoder_layers, batch_size, bi_directional, dropout).to(device)\n",
    "        if(attention!='Yes'):\n",
    "            decoder = DecoderLSTM(target_len, hidden_size, embedding_size, decoder_layers, batch_size, dropout).to(device)\n",
    "    if(attention=='Yes'):\n",
    "        decoder = AttnDecoder(target_len, hidden_size, embedding_size, decoder_layers, batch_size, cell_type, dropout).to(device)\n",
    "        \n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr)\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr)\n",
    "    loss_fun = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "    \n",
    "    # array initialization for storing the different losses for each epochs\n",
    "    \n",
    "    epoch_train_loss = []\n",
    "    epoch_val_loss = []\n",
    "    epoch_val_acc = []\n",
    "    \n",
    "    # Iterate over the epochs\n",
    "    for i in range(0, epochs):\n",
    "        loss, encoder, decoder = train(input_data, encoder, decoder, loss_fun, hidden_size, bi_directional, cell_type, attention, encoder_optimizer, decoder_optimizer, encoder_layers, decoder_layers, batch_size)\n",
    "        val_predictions, val_loss, attn_weights = eval(val_data, encoder, decoder, encoder_layers, decoder_layers, batch_size, hidden_size, bi_directional, cell_type, attention)\n",
    "        \n",
    "        epoch_val_loss.append(val_loss)\n",
    "        epoch_train_loss.append(loss / 51200)  # train_data has 51200 samples\n",
    "        \n",
    "        val_acc = accuracy(val_predictions, val_y)\n",
    "        epoch_val_acc.append(val_acc)\n",
    "        \n",
    "        print(loss / 51200, val_loss, val_acc)\n",
    "    \n",
    "    return epoch_train_loss, epoch_val_loss, epoch_val_acc, encoder, decoder, encoder_layers, decoder_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(input_data, encoder, decoder, encoder_layers, decoder_layers, batch_size, hidden_size, bi_directional, cell_type, attention, build_matrix=False):\n",
    "    with torch.no_grad():\n",
    "        loss_fun = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        total_loss = 0\n",
    "        pred_words = list()\n",
    "        attention_matrix = []\n",
    "        \n",
    "        for x, y in input_data:\n",
    "            attn = []\n",
    "            loss = 0\n",
    "            decoder_words = []\n",
    "            x = x.T\n",
    "            y = y.T\n",
    "            \n",
    "            # Initialize the encoder hidden state\n",
    "            encoder_hidden = encoder.initHidden()\n",
    "            \n",
    "            # Get the number of timesteps in the input sequence\n",
    "            timesteps = len(x)\n",
    "            \n",
    "            if cell_type == 'GRU' or cell_type == 'RNN':\n",
    "                # Run the input sequence through the encoder\n",
    "                encoder_hidden = encoder.initHidden()\n",
    "                encoder_output, encoder_hidden = encoder(x, encoder_hidden)\n",
    "                \n",
    "                if decoder_layers > encoder_layers:\n",
    "                    i = decoder_layers\n",
    "                    decoder_hidden = encoder_hidden\n",
    "                    \n",
    "                    while True:\n",
    "                        if i == encoder_layers:\n",
    "                            break\n",
    "                        # Concatenate the encoder hidden state to match the decoder layers\n",
    "                        decoder_hidden = torch.cat([decoder_hidden, encoder_hidden[-1].unsqueeze(0)], dim=0)\n",
    "                        i -= 1\n",
    "                \n",
    "                elif decoder_layers < encoder_layers:\n",
    "                    decoder_hidden = encoder_hidden[-decoder_layers:]\n",
    "                else:\n",
    "                    decoder_hidden = encoder_hidden\n",
    "                \n",
    "                decoder_input = y[0]\n",
    "                \n",
    "                if bi_directional == \"Yes\":\n",
    "                    # Split the encoder output tensor into two parts along the last dimension\n",
    "                    split_tensor = torch.split(encoder_output, hidden_size, dim=-1)\n",
    "                    # Average the two parts\n",
    "                    encoder_output = torch.add(split_tensor[0], split_tensor[1]) / 2\n",
    "                \n",
    "                # Run the decoder for each timestep in the output sequence\n",
    "                for i in range(0, len(y)):\n",
    "                    if attention == \"Yes\":\n",
    "                        decoder_output, decoder_hidden, attn_weights = decoder(decoder_input, decoder_hidden, encoder_output, len(x))\n",
    "                        max_prob, index = decoder_output.topk(1)\n",
    "                        loss += loss_fun(torch.squeeze(decoder_output), y[i])\n",
    "                        index = index.squeeze()\n",
    "                        decoder_input = index\n",
    "                        decoder_words.append(index.tolist())\n",
    "                        if build_matrix:\n",
    "                            attn.append(attn_weights)\n",
    "                    else:\n",
    "                        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                        max_prob, index = decoder_output.topk(1)\n",
    "                        loss += loss_fun(torch.squeeze(decoder_output), y[i])\n",
    "                        index = index.squeeze()\n",
    "                        decoder_input = index\n",
    "                        decoder_words.append(index.tolist())\n",
    "                \n",
    "                if build_matrix:\n",
    "                    attention_matrix = torch.cat(tuple(x for x in attn), dim=2).to(device)\n",
    "                \n",
    "                decoder_words = np.array(decoder_words)\n",
    "                pred_words.append(decoder_words.T)\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if cell_type == 'LSTM':\n",
    "                # Run the input sequence through the encoder\n",
    "                encoder_hidden = encoder.initHidden()\n",
    "                encoder_state = encoder.initState()\n",
    "                encoder_output, encoder_hidden, encoder_state = encoder(x, encoder_hidden, encoder_state)\n",
    "                \n",
    "                if decoder_layers > encoder_layers:\n",
    "                    i = decoder_layers\n",
    "                    decoder_hidden = encoder_hidden\n",
    "                    decoder_state = encoder_state\n",
    "                    \n",
    "                    while True:\n",
    "                        if i == encoder_layers:\n",
    "                            break\n",
    "                        # Concatenate the encoder hidden state and cell state to match the decoder layers\n",
    "                        decoder_hidden = torch.cat([decoder_hidden, encoder_hidden[-1].unsqueeze(0)], dim=0)\n",
    "                        decoder_state = torch.cat([decoder_state, encoder_state[-1].unsqueeze(0)], dim=0)\n",
    "                        i -= 1\n",
    "                \n",
    "                elif decoder_layers < encoder_layers:\n",
    "                    decoder_hidden = encoder_hidden[-decoder_layers:]\n",
    "                    decoder_state = encoder_state[-decoder_layers:]\n",
    "                else:\n",
    "                    decoder_hidden = encoder_hidden\n",
    "                    decoder_state = encoder_state\n",
    "                \n",
    "                decoder_input = y[0]\n",
    "                \n",
    "                if bi_directional == \"Yes\":\n",
    "                    # Split the encoder output tensor into two parts along the last dimension\n",
    "                    split_tensor = torch.split(encoder_output, hidden_size, dim=-1)\n",
    "                    # Average the two parts\n",
    "                    encoder_output = torch.add(split_tensor[0], split_tensor[1]) / 2\n",
    "                \n",
    "                # Run the decoder for each timestep in the output sequence\n",
    "                for i in range(0, len(y)):\n",
    "                    if attention == \"Yes\":\n",
    "                        decoder_output, decoder_hidden, decoder_state, attn_weights = decoder(decoder_input, decoder_hidden, encoder_output, len(x), decoder_state)\n",
    "                        max_prob, index = decoder_output.topk(1)\n",
    "                        loss += loss_fun(torch.squeeze(decoder_output), y[i])\n",
    "                        index = index.squeeze()\n",
    "                        decoder_input = index\n",
    "                        decoder_words.append(index.tolist())\n",
    "                        if build_matrix:\n",
    "                            attn.append(attn_weights)\n",
    "                    else:\n",
    "                        decoder_output, decoder_hidden, decoder_state = decoder(decoder_input, decoder_hidden, decoder_state)\n",
    "                        max_prob, index = decoder_output.topk(1)\n",
    "                        loss += loss_fun(torch.squeeze(decoder_output), y[i])\n",
    "                        index = index.squeeze()\n",
    "                        decoder_input = index\n",
    "                        decoder_words.append(index.tolist())\n",
    "                \n",
    "                if build_matrix:\n",
    "                    attention_matrix = torch.cat(tuple(x for x in attn), dim=2).to(device)\n",
    "                \n",
    "                decoder_words = np.array(decoder_words)\n",
    "                pred_words.append(decoder_words.T)\n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        predictions = []\n",
    "        for batch in pred_words:\n",
    "            for word in batch:\n",
    "                predictions.append(word)\n",
    "        \n",
    "        return predictions, total_loss / (len(predictions) * len(predictions[0])), attention_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions,y):\n",
    "    count=0\n",
    "    for i in range(0,len(predictions)):\n",
    "        p=predictions[i]\n",
    "        if np.array_equal(p,y[i]):\n",
    "            count+=1\n",
    "    return (count/len(predictions))*100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTEGRATING WITH WANDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wandb_run_sweeps(train_dataset,val_dataset,test_dataset,train_y,val_y,test_y,input_len,target_len):\n",
    "    \n",
    "    config = {\n",
    "        \"project\":\"Assignment3\",\n",
    "        \"method\": 'bayes',\n",
    "        \"metric\": {\n",
    "        'name': 'acc',\n",
    "        'goal': 'maximize'\n",
    "        },\n",
    "        'parameters' :{\n",
    "        \"epochs\": {\"values\":[1]},\n",
    "        \"batchsize\": {\"values\": [64,128,256]},\n",
    "        \"embedding_size\": {\"values\":[256,512,1024]},\n",
    "        \"hidden_size\": {\"values\":[256,512,1024]},\n",
    "        \"encoder_layers\": {\"values\":[2,3,4]},\n",
    "        \"decoder_layers\": {\"values\":[2,3,4]},\n",
    "        \"cell_type\": {\"values\":[\"LSTM\"]},\n",
    "        \"bi_directional\":{\"values\":[\"Yes\",\"No\"]},\n",
    "        \"dropout\":{\"values\":[0.3]},\n",
    "        \"attention\":{\"values\":[\"No\",\"Yes\"]},\n",
    "        }\n",
    "    }\n",
    "    def train_rnn():\n",
    "        wandb.init()\n",
    "\n",
    "        name='_CT_'+str(wandb.config.cell_type)+\"_BS_\"+str(wandb.config.batchsize)+\"_EPOCH_\"+str(wandb.config.epochs)+\"_ES_\"+str(wandb.config.embedding_size)+\"_HS_\"+str(wandb.config.hidden_size)\n",
    "        \n",
    "        val_dataloader=DataLoader(val_dataset,batch_size=wandb.config.batchsize)\n",
    "        test_dataloader=DataLoader(test_dataset,batch_size=wandb.config.batchsize)\n",
    "        train_dataloader=DataLoader(train_dataset,batch_size=wandb.config.batchsize)\n",
    "        \n",
    "        epoch_train_loss,epoch_val_loss,epoch_val_acc,encoder,decoder,encoder_layers,decoder_layers=train_iter(train_dataloader,val_dataloader,val_y,input_len,wandb.config.hidden_size,wandb.config.cell_type,wandb.config.bi_directional,wandb.config.dropout,wandb.config.attention,target_len,wandb.config.epochs,wandb.config.batchsize,wandb.config.embedding_size,wandb.config.encoder_layers,wandb.config.decoder_layers)\n",
    "\n",
    "        for i in range(wandb.config.epochs):\n",
    "            wandb.log({\"val_loss\":epoch_val_loss[i]})\n",
    "            wandb.log({\"val_acc\":epoch_val_acc[i]})\n",
    "            wandb.log({\"loss\":epoch_train_loss[i]})\n",
    "            wandb.log({\"epoch\": (i+1)})\n",
    "        wandb.log({\"validation_accuracy\":epoch_val_acc[-1]})    \n",
    "        \n",
    "        train_predictions,_,_=eval(train_dataloader,encoder,decoder,wandb.config.encoder_layers,\n",
    "                              wandb.config.decoder_layers,wandb.config.batchsize,wandb.config.hidden_size,\n",
    "                              wandb.config.bi_directional,wandb.config.cell_type,wandb.config.attention)\n",
    "\n",
    "        train_accuracy=accuracy(train_predictions,train_y)\n",
    "        wandb.log({\"train_accuracy\":train_accuracy})\n",
    "        \n",
    "        test_predictions,_,_=eval(test_dataloader,encoder,decoder,wandb.config.encoder_layers,\n",
    "                              wandb.config.decoder_layers,wandb.config.batchsize,wandb.config.hidden_size,\n",
    "                              wandb.config.bi_directional,wandb.config.cell_type,wandb.config.attention)\n",
    "\n",
    "        test_accuracy=accuracy(test_predictions,test_y)\n",
    "        wandb.log({\"test_accuracy\":test_accuracy})\n",
    "        wandb.log({\"acc\":epoch_val_acc[-1]})\n",
    "        wandb.run.name = name\n",
    "        wandb.run.save()\n",
    "        wandb.run.finish()\n",
    "    wandb.login(key=\"67fcf10073b0d1bfeee44a1e4bd6f3eb5b674f8e\")\n",
    "    sweep_id=wandb.sweep(config,project=\"Assignment3\")\n",
    "    wandb.agent(sweep_id,function=train_rnn,count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def wandb_run_configuration(train_dataset,val_dataset,test_dataset,train_y,val_y,test_x,test_y,input_len,target_len,epochs,encoder_layers,decoder_layers,batchsize,embedding_size,hidden_size,bi_directional,dropout,cell_type,attention):\n",
    "    \n",
    "    wandb.login(key = \"67fcf10073b0d1bfeee44a1e4bd6f3eb5b674f8e\")\n",
    "    wandb.init(project=\"Assignment3\")\n",
    "    name='_CT_'+str(cell_type)+\"_BS_\"+str(batchsize)+\"_EPOCH_\"+str(epochs)+\"_ES_\"+str(embedding_size)+\"_HS_\"+str(hidden_size)\n",
    "\n",
    "\n",
    "    train_dataloader=DataLoader(train_dataset,batch_size=batchsize)\n",
    "    test_dataloader=DataLoader(test_dataset,batch_size=batchsize)\n",
    "    val_dataloader=DataLoader(val_dataset,batch_size=batchsize)\n",
    "    \n",
    "    epoch_train_loss,epoch_val_loss,epoch_val_acc,encoder,decoder,encoder_layers,decoder_layers=train_iter(train_dataloader,val_dataloader,val_y,input_len,hidden_size,cell_type,bi_directional,dropout,attention,target_len,epochs,batchsize,embedding_size,encoder_layers,decoder_layers)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        wandb.log({\"loss\":epoch_train_loss[i]})\n",
    "        wandb.log({\"val_loss\":epoch_val_loss[i]})\n",
    "        wandb.log({\"val_acc\":epoch_val_acc[i]})\n",
    "        wandb.log({\"epoch\": (i+1)})\n",
    "    wandb.log({\"validation_accuracy\":epoch_val_acc[-1]})    \n",
    "\n",
    "    train_predictions,_,_=eval(train_dataloader,encoder,decoder,encoder_layers,decoder_layers,batchsize,hidden_size,bi_directional,cell_type,attention)\n",
    "\n",
    "    train_accuracy=accuracy(train_predictions,train_y)\n",
    "    wandb.log({\"train_accuracy\":train_accuracy})\n",
    "\n",
    "    test_predictions,_,_=eval(test_dataloader,encoder,decoder,encoder_layers,decoder_layers,batchsize,hidden_size,bi_directional,cell_type,attention)\n",
    "    test_accuracy=accuracy(test_predictions,test_y)\n",
    "    wandb.log({\"test_accuracy\":test_accuracy})\n",
    "    wandb.log({\"acc\":epoch_val_acc[-1]})\n",
    "    \n",
    "    \n",
    "    # test_dataset_attn=MyDataset(test_x[:batchsize],test_y[:batchsize])\n",
    "    # test_dataloader_attn_for_matrix=DataLoader(test_dataset_attn,batch_size=batchsize)\n",
    "    # test_predictions,_,attn_matrix=eval(test_dataloader_attn_for_matrix,encoder,decoder,encoder_layers,decoder_layers,batchsize,hidden_size,bi_directional,cell_type,attention,True)\n",
    "\n",
    "    \n",
    "    # fig=plot_attention(test_predictions,attn_matrix,test_x, idx_to_eng, idx_to_hin)\n",
    "    # fig.savefig(\"ex.png\")\n",
    "    # temp = plt.imread(\"ex.png\")\n",
    "    # plt.show()\n",
    "    # image = wandb.Image(temp)\n",
    "    # wandb.log({\"attention heatmaps\":image})\n",
    "    wandb.run.name = name\n",
    "    wandb.run.save()\n",
    "    wandb.run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\ravis\\.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>M:\\Practice\\Semester-2\\Deep Learning\\Assignments\\Assignment3\\wandb\\run-20240517_234720-wjyac5tv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23m055/Assignment3/runs/wjyac5tv' target=\"_blank\">_CT_LSTM_BS_64_EPOCH_1_ES_256_HS_1024</a></strong> to <a href='https://wandb.ai/cs23m055/Assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs23m055/Assignment3/sweeps/4rzy21dr' target=\"_blank\">https://wandb.ai/cs23m055/Assignment3/sweeps/4rzy21dr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23m055/Assignment3' target=\"_blank\">https://wandb.ai/cs23m055/Assignment3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23m055/Assignment3/sweeps/4rzy21dr' target=\"_blank\">https://wandb.ai/cs23m055/Assignment3/sweeps/4rzy21dr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23m055/Assignment3/runs/wjyac5tv' target=\"_blank\">https://wandb.ai/cs23m055/Assignment3/runs/wjyac5tv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23114452543712796 0.1138137976328532 31.0791015625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>loss</td><td></td></tr><tr><td>test_accuracy</td><td></td></tr><tr><td>train_accuracy</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>validation_accuracy</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>31.0791</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>loss</td><td>0.23114</td></tr><tr><td>test_accuracy</td><td>28.34473</td></tr><tr><td>train_accuracy</td><td>26.88477</td></tr><tr><td>val_acc</td><td>31.0791</td></tr><tr><td>val_loss</td><td>0.11381</td></tr><tr><td>validation_accuracy</td><td>31.0791</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">_CT_LSTM_BS_64_EPOCH_1_ES_256_HS_1024</strong> at: <a href='https://wandb.ai/cs23m055/Assignment3/runs/wjyac5tv' target=\"_blank\">https://wandb.ai/cs23m055/Assignment3/runs/wjyac5tv</a><br/> View project at: <a href='https://wandb.ai/cs23m055/Assignment3' target=\"_blank\">https://wandb.ai/cs23m055/Assignment3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240517_234720-wjyac5tv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    train_df,test_df,val_df,eng_to_idx,hin_to_idx,idx_to_eng,idx_to_hin,input_len,target_len=get_data()\n",
    "\n",
    "    val_x,val_y = pre_process(val_df,eng_to_idx,hin_to_idx)\n",
    "    test_x,test_y = pre_process(test_df,eng_to_idx,hin_to_idx)\n",
    "    train_x,train_y = pre_process(train_df,eng_to_idx,hin_to_idx)\n",
    "    \n",
    "    \n",
    "\n",
    "    train_dataset=MyDataset(train_x,train_y)\n",
    "    test_dataset=MyDataset(test_x,test_y)\n",
    "    val_dataset=MyDataset(val_x,val_y)\n",
    "    \n",
    "    # wandb_run_configuration(train_dataset,val_dataset,test_dataset,train_y,val_y,test_x,test_y,input_len,target_len,idx_to_eng, idx_to_hin,25,4,3,128,512,1024,\"No\",0.3,\"LSTM\",\"Yes\")\n",
    "    # wandb_run_sweeps(train_dataset,val_dataset,test_dataset,train_y,val_y,test_y,input_len,target_len)\n",
    "    wandb_run_configuration(train_dataset,val_dataset,test_dataset,train_y,val_y,test_x,test_y,input_len,target_len, 1,4,3,128,512,1024,\"No\",0.3,\"LSTM\",\"Yes\")\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(test_predictions,attn_matrix,test_x, idx_to_eng, idx_to_hin):\n",
    "    \n",
    "    attn_matrix1=attn_matrix.permute(1,0,2)\n",
    "    attn_matrix1=attn_matrix1[:9]\n",
    "    total_words,input_length,output_length = attn_matrix1.shape\n",
    "\n",
    "\n",
    "    from matplotlib.font_manager import FontProperties\n",
    "\n",
    "\n",
    "    tel_font = FontProperties(fname = '/kaggle/input/hindi-font/TiroDevanagariHindi-Regular.ttf')\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(12,12))\n",
    "\n",
    "    fig.tight_layout(pad=5.0)\n",
    "    fig.subplots_adjust(top=0.90)\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i in range(total_words):\n",
    "        count=0\n",
    "        start1=0\n",
    "        end1=0\n",
    "        eng_word=\"\"\n",
    "        for char in test_x[i]:\n",
    "            if(idx_to_eng[char]=='^'):\n",
    "                start1=count+1\n",
    "            elif(idx_to_eng[char]=='#'):\n",
    "                end1=count\n",
    "                break\n",
    "            else:\n",
    "                eng_word+=idx_to_eng[char]\n",
    "            count+=1\n",
    "\n",
    "        start=0\n",
    "        count=0\n",
    "        hin_word=\"\"\n",
    "        for char in test_predictions[i]:\n",
    "            if(idx_to_hin[char]=='^'):\n",
    "                start=count+1\n",
    "            elif(idx_to_hin[char]=='#'):\n",
    "                end=count\n",
    "                break\n",
    "            else:\n",
    "                hin_word+=idx_to_hin[char]\n",
    "            count+=1\n",
    "\n",
    "        attn=attn_matrix1[i,start1:end1,start:end].cpu().numpy()\n",
    "        sns.heatmap(attn, ax=axes[i],cmap=\"Greens\")\n",
    "        axes[i].set_yticklabels(eng_word,rotation=10)  \n",
    "        axes[i].set_xticklabels(hin_word,fontproperties = tel_font,fontdict={'fontsize':16})\n",
    "        axes[i].xaxis.tick_top()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
